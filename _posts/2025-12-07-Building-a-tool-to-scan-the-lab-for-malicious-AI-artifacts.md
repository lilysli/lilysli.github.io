---
title: "Building a tool to scan the lab for malicious AI artifacts"
layout: post
---

**The problem as an AI hobbyist, is that you might end up downloading malicious AI artifacts.** I‚Äôve downloaded models, Python scripts and Jupyter notebooks from all kinds of places. For example, just from taking a course I would be cloning a Github repository or downloading a model from Hugging Face. There wasn‚Äôt an easy way to check if anything was unsafe.

But since I‚Äôve now created this blog, my first task is: 

‚òëÔ∏è¬†Create an easy way for myself to scan all my downloaded artifacts before running them.

There are many AI artifacts, but the ones I will focus on are:

- Python scripts and Jupyter notebooks
- Their dependencies
- Saved models

**There already exists open-source but siloed tools to scan artifacts.**

For the code (Python scripts and Jupyter notebooks), I can use static analysis. Tools like **Bandit** scan for common insecure patterns (e.g. shell injections via os.system()).  It doesn‚Äôt run the code, but passes it into an Abstract Syntax Tree (AST) and flags bad patterns. 

Still, even if the Python code is deemed secure, it might be using vulnerable libraries. That‚Äôs why Software Composition Analysis (SCA) is another step to check whether the code will pull in packages with known CVEs. A tool like **pip-audit** does this by checking the requirements.txt file and querying the PyPI‚Äôs advisory database to match the packaged against known vulnerabilities. However, it installs the packages temporarily in an isolated environment to resolve dependencies. This can take time if there are a lot of dependencies and they are unpinned.

Serialised models are also risky. Many models are saved using the Python module Pickle, but Pickle enables arbitrary code execution. Just a year ago The JFrog Security Research team found around 100 models on Hugging Face that execute malicious code. A tool that can help prevent these attacks is **ModelScan**, an open source project from Protect AI. It inspects serialised files without loading them to look for unsafe code.

**I built sanityML:** a lightweight CLI tool that scan an entire folder of AI artifacts and gives me an overview of the security issues. The idea of the name, is that it is like a sanity check for an AI hobbyist like me.

sanityML orchestrates the tools bandit, pip-audit, ModelScan and runs them over the target directory to make the process easier. To keep the tool relatively future-proof (and avoid it breaking over an update every time pip-audit tweaks its output format), sanityML streams the raw output directly from the tools. 

I tested the tool on a small controlled folder containing four unsafe artifacts:

(1) Requirements.txt
{% highlight ruby %}
urllib3 < 1.26.5
{% endhighlight %}
This version has CVEs that pip-audit should catch.

(2) malicious_notebook.ipynb
{% highlight ruby %}
import os
os.system('echo API_KEY=abc123')
{% endhighlight %}
Bandit flag should flag shell injections like this.

(3) malicious.py
{% highlight ruby %}
import os
os.system("echo 'Hello from a dangerous shell call'") 
{% endhighlight %}
Similarly to the Jupyter notebook, Bandit should also flag this shell injection.

(4) malicious_model.pkl

This model is generated using the make_malicious_model.py, which defines a MaliciousModel class that tells pickle to run a shell injection upon deserialisation. This injection will only be triggered if someone calls pickle.load(), making it suitable for testing.
{% highlight ruby %}
import pickle
import os

class MaliciousModel:
    def __reduce__(self):
        # This will execute when unpickled
        return (os.system, ("echo 'üö® MALICIOUS CODE EXECUTED ‚Äî API_KEY=SECRET123'",))

with open("malicious_model.pkl", "wb") as f:
    pickle.dump(MaliciousModel(), f)

print("‚úÖ malicious_model.pkl created") 
{% endhighlight %}

The model will be generated by running:
`python make_malicious_model.py.`

**Running the sanityML against my test folder yielded positive results.**

First it discovered all the artifacts in the test folder.

![sanityML discovery output](/assets/images/sanityML/sanityML-1.png)

Then it scans the Python script, Jupyter notebook (after converting it to Python), requirements.txt and the model.

![sanityML discovery output](/assets/images/sanityML/sanityML-2.png)
![sanityML discovery output](/assets/images/sanityML/sanityML-3.png)
![sanityML discovery output](/assets/images/sanityML/sanityML-4.png)

The tool detected all the expected vulnerabilities.


However, this tool has its limitations:

- pip-audit only finds known CVEs, not logic bugs or zero-days.
- ModelScan only covers deserialization risks, but AI models bring many more attack types, such as model poisoning
- When downloading Python scripts, notebooks, models etc. it is critical to make a judgement of whether the source is trustworthy. I wouldn‚Äôt rely on sanityML alone to protect my lab.

If you think this sounds useful, you can try it on: [https://github.com/lilysli/sanityml](https://github.com/lilysli/sanityml)

**Sources:** 

[https://jfrog.com/blog/data-scientists-targeted-by-malicious-hugging-face-ml-models-with-silent-backdoor/](https://jfrog.com/blog/data-scientists-targeted-by-malicious-hugging-face-ml-models-with-silent-backdoor/)

