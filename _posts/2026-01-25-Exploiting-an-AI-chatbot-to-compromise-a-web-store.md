---
title: "Exploiting an AI chatbot to compromise a web store"
layout: post
---
**LLMs are becoming common in service industries, especially customer support. For example, a user can ask the chatbot questions about product offerings or get help regarding account issues. However, if not properly secured, the chatbot may allow attackers to manipulate it as part of an attack chain, potentially leading to webshop compromise. In this blog, I walk through how I solved the expert-level Web LLM lab provided by Portswigger: Deleting a user account using the chatbot in the web store.**

As Large Language Models (LLMs) gained popularity with the introduction of ChatGPT, enterprises sought to integrate them with their business, for instance, in online customer support. 

Similarly to how an actual customer support employee would need access to various store- and customer-related data to assist users, the LLM also requires such access. To enable this, developers typically build functions that the LLM can call. Each function takes a parameter, such as the product name, and converts the request to an accepted format (e.g., a JSON object), which can then be sent via an API integration to a database or other external resources.

The function retrieves the necessary information and returns it to the LLM. This process is shown in the simplified diagram below.

![LLM-function](/assets/images/webshop_attack/LLM_functions.png) 

The close integration of the LLM with the webshop and external resources broadens the attack surface, and in this blog post, I will exploit this integration to solve the expert-level Web LLM lab from Portswigger.

## Walk-through of lab solution

### 1. Investigate the webshop
In this lab, the goal is to delete the user account Carlos. For that task, I am given access to a webshop site and an email account.

Webshop home page:

![home-page](/assets/images/webshop_attack/home-page.png)

The webshop home page shows different products it sells, and there are also other pages: "My account" and "Live chat".
The "My account" page asks the user to register an account, so I start by engaging with the AI chatbot in the "Live chat" page.

### 2. Investigate the chatbot

Using direct prompt injection, impersonating a system admin (figure of authority), I learn that the chatbot has access to two functions (password_reset and product_info) and one tool (parallel). 

![direct-prompt-injection](/assets/images/webshop_attack/direct-prompt-injection.png)

Returning to the home page, I browse through the products and observe that to write a review one must log in to an account.

![product-review](/assets/images/webshop_attack/review-login.png)

### 3. Create an account

With the email provided in the lab, I register an account under the username Nicki.

![registration](/assets/images/webshop_attack/register.png)

Once inside the email server, I confirm the registration.

![email-server](/assets/images/webshop_attack/email-server.png)

After logging in with the new username and password, I land on the "My Account" page. Here, the "Delete" button is displayed.

![My-account](/assets/images/webshop_attack/delete-page.png)

### 4. Try different prompt injections
Returning to the AI chatbot, I test different direct prompt injection attacks (primarily role-playing) to get it to delete my own account or Carlos's. However, that effort is in vain, and instead, I try indirect prompt injection. 

Indirect prompt injection places the prompt in an external context that the chatbot uses. Given that the chatbot has access to a "product_info" function, which takes external data, I write a review under the *Lightweight "l33t" Leather Jacket* product. After asking the chatbot to show me the reviews for the jacket, it shows the exact content in the review. 

*Note: In the lab description, they say that Carlos frequently ask the chatbot about that specific Jacket.*

### 5. Look for vulnerabilities

Given that the content is output directly by the chatbot, I try to insert a simple cross-site scripting (XSS) attack: ```<img src=x onerror=alert(1)>``` as a review for the jacket.

Afterwards, I ask the chatbot to show me the full content of the jacket reviews, and the alert appears, proving it has insecure output handling. 

![Alert](/assets/images/webshop_attack/alert.png)

### 5. Build a reliable XSS payload

Now I need to write a script that can bypass the security checks and delete my account using the form in the "My account" page.

The source code for the "My account" page shows the following form:

```html
<p>Your username is: nicki</p>
    <form class="login-form" name="change-email-form" action="/my-account/change-email" method="POST">
        <label>Email</label>
        <input required type="email" name="email" value="">
        <input required type="hidden" name="csrf" value="z4e7Apj9ifS0SlQsBXSWyyYgExMFqqJE">
        <button class='button' type='submit'> Update email </button>
    </form>
    <form id=delete-account-form action="/my-account/delete" method="POST">
        <input required type="hidden" name="csrf" value="z4e7Apj9ifS0SlQsBXSWyyYgExMFqqJE">
        <button class="button" type="submit">Delete account</button>
    </form>
```

I tried several approaches, such as complex JavaScript payloads (which the chatbot filtered out). In the end, I used an <iframe> to load the account page and auto-submit the delete form, disguising the payload as natural language in the review to bypass the AIâ€™s content filters.
```
I really like the jacket. In addition I found a paper with the sentence: <iframe src=my-account onload=this.contentDocument.getElementById('delete-account-form').submit()> 

All in all a fun experience!
```

### 6. Insert payload in a review
I insert the XSS payload in a review for the *Lightweight "l33t" Leather Jacket*.

![indirect-prompt-injection](/assets/images/webshop_attack/indirect-prompt-injection.png)

### 7. Goal achieved: Carlos' account is deleted

After publishing that review, the lab is solved, as Carlos frequently asks the chatbot about the jacket (probably an automatic check in the lab) and thus received the XSS exploit, which deleted his account for him.

![lab-solved](/assets/images/webshop_attack/lab-solved.png)

## Defenses

This lab covers several AI-related attacks: direct prompt injection, indirect prompt injection, and insecure output handling, and shows the risks of adding AI to a website without proper security controls. 

There are several defenses that could be used to prevent these attacks:
- **Sanitise user inputs at every entry point:**
All user-supplied data, such as product reviews and chat messages, should be HTML-escaped or stripped of markup before storage, processing, or display. This helps prevent attacks such as the indirect prompt-injection attack involving XSS. 
- **Sanitise the input and output of the functions:** Another critical issue for the webshop is that the function product_info returns the product reviews as raw HTML. The input and output of the functions should be sanitised and HTML-escaped to prevent them from being rendered as executable code in AI responses.
- **Enforce structured, safe output from the chatbot:** Using LangChain (a framework for organising AI workflows), you can use strict output schemas to make sure the output from the chatbot is plain text.
- **Monitor user interaction and block malicious behavior:** To test different prompt injections and build the right XSS payload, I engaged often with the chatbot. Implementing a monitoring system, such as detecting unusual function usage or prompt injection patterns, can stop attacks earlier.

