---
title: "How can we protect our AI lab from accidental malware?"
layout: post
---

**In recent years, the number of people studying and experimenting with AI has grown rapidly. Without safeguards, they accidentally end up downloading malware hidden in free AI models and Python scripts. In this blog, I explore current solutions for scanning AI artifacts for malware and vulnerabilities, and build my own streamlined tool to protect my lab.**

The problem as an AI hobbyist is that you might accidentally download malicious AI artifacts. I’ve downloaded models, Python scripts, and Jupyter notebooks from all kinds of places. For example, just from taking a course, I might clone a GitHub repository or download a model from Hugging Face. There wasn’t an easy way to check if anything was unsafe.

Now that I’ve created this blog, my first task is to create an easy way to scan all my downloaded artifacts before running them.

There are many AI artifacts, but the ones I will focus on are:

* Python scripts and Jupyter notebooks
* Their dependencies
* Saved models

**Open-source tools for scanning artifacts already exist, but they remain siloed.**

For the code (Python scripts and Jupyter notebooks), I can use static analysis. Tools like Bandit scan for common insecure patterns (e.g., shell injections via os.system()).  It doesn’t run the code; instead, it passes it into an Abstract Syntax Tree (AST) and flags bad patterns.

Even if the Python code is deemed secure, it may still rely on vulnerable libraries. That’s why Software Composition Analysis (SCA) is another step to check whether code pulls in packages with known CVEs. A tool like pip-audit does this by checking the requirements.txt file and querying PyPI’s advisory database to match the packages against known vulnerabilities. However, it installs the packages temporarily in an isolated environment to resolve dependencies. This can take time if there are many dependencies and they are unpinned.

Serialised models are also risky. Many models are saved using the Python module Pickle, but Pickle enables arbitrary code execution. Just a year ago, the JFrog Security Research team found around 100 Hugging Face models that execute malicious code. A tool that can help prevent these attacks is ModelScan, an open-source project from Protect AI. It inspects serialised files without loading them to look for unsafe code.

**I built sanityML, a lightweight CLI tool that scans entire folders of AI artifacts and provides security issue overviews.** The name refers to a "sanity check" for AI hobbyists like me.

sanityML orchestrates the tools bandit, pip-audit, and ModelScan, running them over the target directory to simplify the process. To keep the tool relatively future-proof (and avoid it breaking with every pip-audit update that tweaks its output format), sanityML streams the raw output directly from the tools. 

I tested the tool on a small controlled folder containing four unsafe artifacts:

(1) Requirements.txt
{% highlight ruby %}
urllib3 < 1.26.5
{% endhighlight %}
This version has CVEs that pip-audit should catch.

(2) malicious_notebook.ipynb
{% highlight ruby %}
import os
os.system('echo API_KEY=abc123')
{% endhighlight %}
Bandit flag should flag shell injections like this.

(3) malicious.py
{% highlight ruby %}
import os
os.system("echo 'Hello from a dangerous shell call'") 
{% endhighlight %}
Similarly to the Jupyter notebook, Bandit should also flag this shell injection.

(4) malicious_model.pkl

This model is generated using the make_malicious_model.py, which defines a MaliciousModel class that tells pickle to run a shell injection upon deserialisation. This injection will only be triggered if someone calls pickle.load(), making it suitable for testing.
{% highlight ruby %}
import pickle
import os

class MaliciousModel:
    def __reduce__(self):
        # This will execute when unpickled
        return (os.system, ("echo 'MALICIOUS CODE EXECUTED — API_KEY=SECRET123'",))

with open("malicious_model.pkl", "wb") as f:
    pickle.dump(MaliciousModel(), f)

print("malicious_model.pkl created") 
{% endhighlight %}

The model will be generated by running:
`python make_malicious_model.py.`

**Running the sanityML against my test folder yielded positive results.**

First it discovered all the artifacts in the test folder.

![sanityML discovery output](/assets/images/sanityML/sanityML-1.png)

Then it scans the Python script, Jupyter notebook (after converting it to Python), requirements.txt and the model.

![sanityML discovery output](/assets/images/sanityML/sanityML-2.png)
![sanityML discovery output](/assets/images/sanityML/sanityML-3.png)
![sanityML discovery output](/assets/images/sanityML/sanityML-4.png)

The tool detected all the expected vulnerabilities.

However, this tool has its limitations:
- pip-audit only finds known CVEs, not logic bugs or zero-days.
- ModelScan only covers deserialization risks, but AI models bring many more attack types, such as model poisoning
- When downloading Python scripts, notebooks, models, etc., it is critical to make a judgment of whether the source is trustworthy. I wouldn’t rely on sanityML alone to protect my lab.

If you think this sounds useful, you can try it on: [https://github.com/lilysli/sanityml](https://github.com/lilysli/sanityml)

**Sources:** 

[https://jfrog.com/blog/data-scientists-targeted-by-malicious-hugging-face-ml-models-with-silent-backdoor/](https://jfrog.com/blog/data-scientists-targeted-by-malicious-hugging-face-ml-models-with-silent-backdoor/)

